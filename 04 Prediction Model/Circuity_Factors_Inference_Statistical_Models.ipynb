{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-25T10:55:24.232258",
     "start_time": "2016-11-25T10:55:12.443265"
    },
    "collapsed": true
   },
   "source": [
    "# Circuity Factors - Inference Model\n",
    "\n",
    "\n",
    "*MIT Megacity Logistics Lab*(c)\n",
    "\n",
    "- Andr√© Snoeck <asnoeck@mit.edu>\n",
    "+ Daniel Merchan <dmerchan@mit.edu>\n",
    "\n",
    "**This code may contain confidential and sensitive information. Please do not distribute.**\n",
    "\n",
    "## Summary\n",
    "\n",
    "The aim of this code is to explore different statistical models to predict the circuity factor using a specific set of predictors. After developing several different models to describe the data, we are able to evaluate the performance of those models and make a trade-off about the most appropriate model for our particular dataset.\n",
    "\n",
    "The code can be split into two parts. \n",
    "\n",
    "### General functions\n",
    "\n",
    "The first part of the code defines general functions, that are not tied to a particular dataset. Those functions are based on the methods we learned during Harvard's Data Science class, AC209a, and we aim to use those in other projects as well. \n",
    "\n",
    "The first set of functions are data preparation functions\n",
    "- Load data\n",
    "- Split data - To create a training and testing set\n",
    "- Create x & y - To split the predictors and the independent variable\n",
    "- Standardize data\n",
    "- Polynomial transformation\n",
    "\n",
    "The second set of functions creates different kinds of prediction models based on the test set. A general structure is followed for these models. Based on cross validation on the training set, the optimal tuning parameters are determined. As a result, the function will return the configuration of the model that best describes our data. \n",
    "\n",
    "We implemented the following numerical prediction models\n",
    "- Linear regression\n",
    "- Ridge and lasso regression (tuning parameter is alpha)\n",
    "- Random forests (tuning parameters are number of trees and tree depth)\n",
    "\n",
    "Furthermore, we implemented functions for forward and backward stepwise subset selection. \n",
    "\n",
    "For classification, we implemented the following models\n",
    "- Logistics regression (tuning parameter is C)\n",
    "- Random forests (tuning parameters are number of trees and tree depth)\n",
    "\n",
    "Naturally, these models can be fed with different predictors, including polynomial combinations of the predictors. \n",
    "\n",
    "For a more in depth look at the functions for the different models, we refer to the doc strings. \n",
    "\n",
    "### Circuity factor application \n",
    "\n",
    "In this application, we use the functions described above to compare the performance of different models in terms of descriptive and predictive power. Furthermore, we will use the results of this data to analyze the effect of the different predictors that account for circuity in a pixel. \n",
    "\n",
    "We analyze both regression as well as classification models. Often, for network planning, practitioners will be interested in the actual value of circuity factor. However, if policy makers at the local government want to understand and influence the circuity factors and in practice it is impossible to develop urban planning initiatives for each individual pixel. A more natural way for practitioners to digest the large amount of pixels is to think of them in clusters of pixels with similar characteristics and the government will set-up different urban planning strategies for each cluster of pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.448222",
     "start_time": "2016-12-14T22:23:19.245299"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#General  libraries\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 30\n",
    "import scipy as sp\n",
    "import copy\n",
    "\n",
    "#Specific data science packages\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import LogisticRegression as Log_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.tree import DecisionTreeRegressor as Reg_Tree\n",
    "from sklearn.ensemble import RandomForestRegressor as Reg_Forest\n",
    "from sklearn.ensemble import RandomForestClassifier as Cls_Forest\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS as OLS\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures as PolyFeat\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "\n",
    "from sklearn.model_selection import KFold as KFold\n",
    "from sklearn.model_selection import cross_val_score as CV\n",
    "from sklearn.model_selection import train_test_split as Split\n",
    "from sklearn.feature_selection import f_regression as RegTest\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#Visualization libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#System libraries\n",
    "from itertools import combinations\n",
    "import itertools as it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.455767",
     "start_time": "2016-12-14T22:23:20.451099"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    To read the dataframe\n",
    "    Input:\n",
    "    - filename: variable with the name of the file to be read\n",
    "    \n",
    "    Output:\n",
    "    - df: file converted to a pandas dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.463560",
     "start_time": "2016-12-14T22:23:20.458124"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(df, m_test_set):\n",
    "    '''\n",
    "    Split a dataframe in training and testing sets. Parameter m sets the frraction of data\n",
    "    allocated to the test set (suggested = 1/3 or 1/2)\n",
    "    \n",
    "    Input:\n",
    "    - df (m x n): Dataframe of data including predictors and response variable\n",
    "    - m_test_set: Fraction of data to be allocated to the testing set. \n",
    "    \n",
    "    Output:\n",
    "    - df_train (m x n): Dataframe for training the model \n",
    "    - df_test (m x n): Dataframe for testing the model\n",
    "    '''\n",
    "\n",
    "    df_train, df_test = Split(df, test_size = m_test_set)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.472172",
     "start_time": "2016-12-14T22:23:20.466387"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_x_y(dataframe):\n",
    "    \"\"\"\n",
    "    Split the dataframe into two numpy arrays with predictors and indpendent variable\n",
    "    \n",
    "    Input:\n",
    "    - dataframe: Dataframe (m x n) with any number of predictors and one independent variable in the last column [x|y]\n",
    "    \n",
    "    Output:\n",
    "    - x (m x (n-1)): Numpy array with predictors\n",
    "    - y (m x 1):     Numpy array with independent variable \n",
    "    \"\"\"\n",
    "    \n",
    "    x = dataframe.values[:, :-1]\n",
    "    y = dataframe.values[:, -1]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.479976",
     "start_time": "2016-12-14T22:23:20.474582"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_data(x):\n",
    "    \"\"\"\n",
    "    Scale/standardize a matrix of (mxn) predictors. Each predictor array will be scaled to \n",
    "    zero mean and unit variance.\n",
    "    \n",
    "    Input: \n",
    "    - x (m x n): Numpy array with predictors \n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    - x_scaled: Standarized numpy array\n",
    "    - scaler: Standarization model that can be re-used to standarize other datasets\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = Standardize().fit(x)\n",
    "    \n",
    "    x_scaled = scaler.transform(x)\n",
    "    \n",
    "    return x_scaled, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.488691",
     "start_time": "2016-12-14T22:23:20.482568"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_transf(x, deg):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform a matrix of (mxn) predictors to a given polynomial degree\n",
    "    \n",
    "    Input: \n",
    "    - x (m x n): Numpy array with predictors \n",
    "    - deg: Desired polynomial degree\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    - x_transf: Numpy array with predictors transformend to the given polynomial degree. Includes all polynomial combinations of the features \n",
    "    with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], \n",
    "    the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
    "    \"\"\"\n",
    "    \n",
    "    #defining the polynomial degree\n",
    "    poly = PolyFeat(deg)\n",
    "    \n",
    "    #trasnforming x to the desired polynomial degree\n",
    "    x_transf = poly.fit_transform(x) \n",
    "\n",
    "    return x_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.498946",
     "start_time": "2016-12-14T22:23:20.491077"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_linear_regression_model(x, y, print_results = False):\n",
    "    \"\"\"\n",
    "    Create linear regression model that best fits the input dataframe\n",
    "    \n",
    "    Input:\n",
    "    - x: Dataframe (m x n) with m observations and n predictors\n",
    "    - y: Dataframe (m x 1) with m observations and 1 independent variable\n",
    "    \n",
    "    Output:\n",
    "    - lin_reg_model: Fitted linear regression model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split dataframe\n",
    "    #x, y = create_x_y(dataframe)\n",
    "    \n",
    "    # Create linear regression model\n",
    "    lin_reg_model = Lin_Reg()\n",
    "    lin_reg_model.fit(x, y)\n",
    "    \n",
    "    # Print some results\n",
    "    if print_results:\n",
    "        print '\\nLinear regression model created'\n",
    "        print 'Train R^2: ', format(lin_reg_model.score(x, y), '.2f')\n",
    "    \n",
    "    return lin_reg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Ridge and Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.536451",
     "start_time": "2016-12-14T22:23:20.501644"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_regularized_regression_model(x,y, regularization_type = 'Ridge', print_results = False, visualize_results = False):\n",
    "    \"\"\"\n",
    "    Create ridge regression model that best fits the input dataframe\n",
    "    \n",
    "    Input:\n",
    "    - x: Dataframe (m x n) with m observations and n predictors\n",
    "    - y: Dataframe (m x 1) with m observations and 1 independent variable\n",
    "    - regularization_type: Either 'Ridge' or 'Lasso', default is 'Ridge'\n",
    "    - print results: Optional to show best regularization parameter and associated CV and Training set scores\n",
    "    - visualize_results: Optional to show best regularization parameter search\n",
    "    \n",
    "    Output:\n",
    "    - ridge_reg_model: Fitted ridge regression model optimized for the tuning parameter lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split dataframe\n",
    "    #x, y = create_x_y(dataframe)\n",
    "    \n",
    "    # Create evaluation parameter range\n",
    "    min_pow_10 = -7\n",
    "    max_pow_10 = 7\n",
    "    reg_param_range = [ 10**i for i in range(min_pow_10, max_pow_10)]\n",
    "    \n",
    "    # Create list to keep track of scores\n",
    "    score_train = []\n",
    "    score_CV = []\n",
    "    \n",
    "    # Iterate over values for regression parameter\n",
    "    for reg_param in reg_param_range:\n",
    "        \n",
    "        # Create model\n",
    "        if regularization_type == 'Ridge':\n",
    "            model = Ridge_Reg(alpha = reg_param)\n",
    "        elif regularization_type == 'Lasso':\n",
    "            model = Lasso_Reg(alpha = reg_param)\n",
    "        else:\n",
    "            print 'Provide either ridge or lasso'\n",
    "            \n",
    "        # Find cross validation score\n",
    "        score_CV.append(np.mean(CV(model, x, y, cv = 5)))\n",
    "        \n",
    "        # Find score on training set\n",
    "        model.fit(x,y)\n",
    "        score_train.append(model.score(x, y))\n",
    "    \n",
    "    # Find highest CV score\n",
    "    highest_CV_score = np.max(score_CV)\n",
    "    \n",
    "    # Find alpha related to highest score\n",
    "    best_reg_param = reg_param_range[np.argmax(score_CV)]\n",
    "    \n",
    "    # Create and fit model with best regularization parameter\n",
    "    if regularization_type == 'Ridge':\n",
    "        model = Ridge_Reg(alpha = best_reg_param)\n",
    "    elif regularization_type == 'Lasso':\n",
    "        model = Lasso_Reg(alpha = best_reg_param)\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    # Print some results\n",
    "    if print_results:\n",
    "        print '\\n', regularization_type,  'regression model created'\n",
    "        print 'Best regularization parameter:', best_reg_param\n",
    "        print 'Train R^2: ', format(model.score(x, y), '.2f')\n",
    "        print 'CV R^2: ', format(np.max(score_CV), '.2f')\n",
    "    \n",
    "    # Visualize search\n",
    "    if visualize_results:\n",
    "        visualize_regularized_regression(score_train, score_CV, reg_param_range, regularization_type + ' regression')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-04T14:54:38.633841",
     "start_time": "2016-12-04T14:54:38.630503"
    }
   },
   "source": [
    "##### Visualize regularized regression parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.549874",
     "start_time": "2016-12-14T22:23:20.538899"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_regularized_regression(train_scores, cv_scores, reg_param_range, title):\n",
    "    \"\"\"\n",
    "    Visualize the search for the best regularization parameter\n",
    "    \n",
    "    Input:\n",
    "    - train_scores: List with values of the model scores on the training set\n",
    "    - cv_scores: List with values of the model scores for cross validation\n",
    "    - reg_param_range: List with values for regression parameter\n",
    "    - title: String with title above plot\n",
    "    \n",
    "    Output:\n",
    "    - Visualization of parameter search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot train an test R-squared as a function parameter value\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    ax.semilogx(reg_param_range, \n",
    "                train_scores, \n",
    "                c='b', \n",
    "                label='Train scores')\n",
    "    ax.semilogx(reg_param_range, \n",
    "                cv_scores, \n",
    "                c='r', \n",
    "                label='CV scores')\n",
    "\n",
    "    ax.set_xlabel('Regularization parameter')\n",
    "    ax.set_ylabel('R^2 score')\n",
    "    ax.set_ylim((min(cv_scores) - 0.2, 1.2))\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward stepwise subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.598104",
     "start_time": "2016-12-14T22:23:20.552228"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_stepwise_subset_selection(x, y, predictor_list, print_results = False):\n",
    "    \"\"\"\n",
    "    Select best subset of predictors based on BIC score to find the subset of most relevant variables. \n",
    "    \n",
    "    Input:\n",
    "    - x: Dataframe (m x n) with m observations and n predictors\n",
    "    - y: Dataframe (m x 1) with m observations and 1 independent variable\n",
    "    predictor_names: List with names of predictors\n",
    "    \n",
    "    Output:\n",
    "    - best_predictors: List of best predictors\n",
    "    \"\"\"\n",
    "    \n",
    "    predictor_names = copy.deepcopy(predictor_list)\n",
    "    \n",
    "    x = sm.add_constant(x, prepend=False)\n",
    "    predictor_names.append('Constant')\n",
    "\n",
    "    ### Step-wise Forward Selection\n",
    "    d = x.shape[1] # total no. of predictors\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    # Keep track of current set of chosen predictors, and the remaining set of predictors\n",
    "    current_predictors = [] \n",
    "    remaining_predictors = range(d)\n",
    "\n",
    "    # Set some initial large value for min BIC score for all possible subsets\n",
    "    best_r_squared = 0\n",
    "    best_adjusted_r_squared = 0\n",
    "        \n",
    "    # Keep track of the best subset of predictors\n",
    "    best_subset = [] \n",
    "\n",
    "    # Iterate over all possible subset sizes, 0 predictors to d predictors\n",
    "    for size in range(d):    \n",
    "        max_r_squared = -1e10 # set some initial small value for max R^2\n",
    "        best_predictor = -1 # set some throwaway initial number for the best predictor to add\n",
    "        adjusted_r_squared_with_best_predictor = 0 # set some initial large value for BIC score   \n",
    "\n",
    "        # Iterate over all remaining predictors to find best predictor to add\n",
    "        for i in remaining_predictors:\n",
    "            # Make copy of current set of predictors\n",
    "            temp = current_predictors[:]\n",
    "            # Add predictor 'i'\n",
    "            temp.append(i)\n",
    "\n",
    "            # Use only a subset of predictors in the training data\n",
    "            x_subset = x[:, temp]\n",
    "            p = x_subset.shape[1]\n",
    "            # Fit and evaluate R^2\n",
    "            lin_reg_model = Lin_Reg()\n",
    "            lin_reg_model.fit(x_subset, y)\n",
    "            r_squared = lin_reg_model.score(x_subset, y)\n",
    "            adjusted_r_squared = (1 - (1 - r_squared)*(N - 1)/\n",
    "                                  (N - p - 1))\n",
    "            \n",
    "            # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "            if(r_squared > max_r_squared):\n",
    "                max_r_squared = r_squared\n",
    "                best_predictor = i\n",
    "                adjusted_r_squared_with_best_predictor = adjusted_r_squared\n",
    "                \n",
    "        # Remove best predictor from remaining list, and add best predictor to current list\n",
    "        remaining_predictors.remove(best_predictor)\n",
    "        current_predictors.append(best_predictor)\n",
    "\n",
    "        # Check if BIC for with the predictor we just added is lower than \n",
    "        # the global minimum across all subset of predictors\n",
    "        if(adjusted_r_squared_with_best_predictor > best_adjusted_r_squared):\n",
    "            best_subset = current_predictors[:]\n",
    "            best_r_squared = max_r_squared\n",
    "            best_adjusted_r_squared = adjusted_r_squared_with_best_predictor\n",
    "    \n",
    "    if print_results:\n",
    "        best_predictors = [predictor_names[i] for i in best_subset]\n",
    "        print 'Step-wise forward subset selection:'\n",
    "        print 'Train R^2: ', format(best_r_squared, '.2f')\n",
    "        print 'Predictors: ', best_predictors\n",
    "    \n",
    "    return best_subset # add 1 as indices start from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T19:49:46.523453",
     "start_time": "2016-12-06T19:49:46.520347"
    }
   },
   "source": [
    "#### Backward stepwise subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.653346",
     "start_time": "2016-12-14T22:23:20.600361"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_stepwise_subset_selection(x, y, predictor_list, print_results = False):\n",
    "    \"\"\"\n",
    "    Select best subset of predictors based on BIC score to find the subset of most relevant variables. \n",
    "    \n",
    "    Input:\n",
    "    - x: Dataframe (m x n) with m observations and n predictors\n",
    "    - y: Dataframe (m x 1) with m observations and 1 independent variable\n",
    "    predictor_names: List with names of predictors\n",
    "    \n",
    "    Output:\n",
    "    - best_predictors: List of best predictors\n",
    "    \"\"\"\n",
    "    \n",
    "    predictor_names = copy.deepcopy(predictor_list)\n",
    "    \n",
    "    x = sm.add_constant(x, prepend=False)\n",
    "    predictor_names.append('Constant')\n",
    "\n",
    "    ###  Step-wise Backward Selection\n",
    "    d = x.shape[1] # total no. of predictors\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    # Keep track of current set of chosen predictors\n",
    "    current_predictors = range(d)\n",
    "\n",
    "    # First, fit and evaluate BIC using all 'd' number of predictors\n",
    "    p = x.shape[1]\n",
    "\n",
    "    # Fit and evaluate R^2\n",
    "    lin_reg_model = Lin_Reg()\n",
    "    lin_reg_model.fit(x, y)\n",
    "    r_squared = lin_reg_model.score(x, y)\n",
    "    adjusted_r_squared = (1 - (1 - r_squared)*(N - 1)/\n",
    "                          (N - p - 1))\n",
    "\n",
    "    # Set the minimum BIC score, initially, to the BIC score using all 'd' predictors\n",
    "    # Keep track of the best subset of predictors\n",
    "    best_subset = current_predictors[:] \n",
    "    best_adjusted_r_squared = adjusted_r_squared\n",
    "    \n",
    "    # Iterate over all possible subset sizes, d predictors to 1 predictor\n",
    "    for size in range(d - 1, 1, -1): # stop before 0 to avoid choosing an empty set of predictors\n",
    "        max_r_squared = -1e10 # set some initial small value for max R^2\n",
    "        worst_predictor = -1 # set some throwaway initial number for the worst predictor to remove\n",
    "        adjusted_r_squared_with_best_predictor = 0  # set some initial large value for min BIC score  \n",
    "\n",
    "        # Iterate over current set of predictors (for potential elimination)\n",
    "        for i in current_predictors:\n",
    "            # Create copy of current predictors, and remove predictor 'i'\n",
    "            temp = current_predictors[:]\n",
    "            temp.remove(i)\n",
    "\n",
    "            # Use only a subset of predictors in the training data\n",
    "            x_subset = x[:, temp]\n",
    "            p = x_subset.shape[1]\n",
    "            \n",
    "            # Fit and evaluate R^2\n",
    "            lin_reg_model = Lin_Reg()\n",
    "            lin_reg_model.fit(x_subset, y)\n",
    "            r_squared = lin_reg_model.score(x_subset, y)\n",
    "            adjusted_r_squared = (1 - (1 - r_squared)*(N - 1)/\n",
    "                                  (N - p - 1))\n",
    "\n",
    "            # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "            if(r_squared > max_r_squared):\n",
    "                max_r_squared = r_squared\n",
    "                worst_predictor = i\n",
    "                adjusted_r_squared_with_best_predictor = adjusted_r_squared\n",
    "\n",
    "        # Remove worst predictor from current set of predictors\n",
    "        current_predictors.remove(worst_predictor)\n",
    "\n",
    "        # Check if BIC for with the predictor we just added is lower than \n",
    "        # the global minimum across all subset of predictors\n",
    "        if(adjusted_r_squared_with_best_predictor > best_adjusted_r_squared):\n",
    "            best_subset = current_predictors[:]\n",
    "            best_r_squared = max_r_squared\n",
    "            best_adjusted_r_squared = adjusted_r_squared_with_best_predictor\n",
    "\n",
    "    if print_results:\n",
    "        best_predictors = [predictor_names[i] for i in best_subset]\n",
    "        print 'Step-wise backward subset selection:'\n",
    "        print 'Train R^2: ', format(best_r_squared, '.2f')\n",
    "        print 'Subset of best predictors: ', best_predictors\n",
    "    \n",
    "    return best_subset # add 1 as indices start from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.697499",
     "start_time": "2016-12-14T22:23:20.655969"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_logistic_regression_model(x,y, regularization_type = 'l2', print_results = False, visualize_results = False):\n",
    "    \"\"\"\n",
    "    Create logistic regression model that best fits the input dataframe\n",
    "    \n",
    "    Input:\n",
    "    - x: Dataframe (m x n) with m observations and n predictors\n",
    "    - y: Dataframe (m x 1) with m observations and 1 independent variable\n",
    "    - regularization_type: Either 'l2' or 'l1', default is 'l1'\n",
    "    - print results: Optional to show best regularization parameter and associated CV and Training set scores\n",
    "    - visualize_results: Optional to show best regularization parameter search\n",
    "    \n",
    "    Output:\n",
    "    - model: Fitted logistic regression model optimized for the tuning parameter lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create evaluation parameter range\n",
    "    min_pow_10 = -7\n",
    "    max_pow_10 = 7\n",
    "    reg_param_range = [ 10**i for i in range(min_pow_10, max_pow_10)]\n",
    "    \n",
    "    # Create list to keep track of scores\n",
    "    score_train = []\n",
    "    score_CV = []\n",
    "    \n",
    "    # Try to beat best score\n",
    "    best_score = 0\n",
    "    \n",
    "    # Iterate over values for regression parameter\n",
    "    for reg_param in reg_param_range:\n",
    "        \n",
    "        # Create model\n",
    "        if regularization_type == 'l2':\n",
    "            model = Log_Reg(penalty = regularization_type, C = reg_param)\n",
    "        elif regularization_type == 'l1':\n",
    "            model = Log_Reg(penalty = regularization_type, C = reg_param)\n",
    "        else:\n",
    "            print 'Provide either ridge or lasso'\n",
    "            \n",
    "        # Find cross validation score\n",
    "        average_score = np.mean(CV(model, x, y, cv = 5))\n",
    "        score_CV.append(np.mean(CV(model, x, y, cv = 5)))\n",
    "        \n",
    "        # Find score on training set\n",
    "        model.fit(x,y)\n",
    "        score_train.append(model.score(x, y))\n",
    "        \n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_reg_param = reg_param\n",
    "            \n",
    "    #     # Find highest CV score\n",
    "    #     highest_CV_score = np.max(score_CV)\n",
    "\n",
    "    #     # Find alpha related to highest score\n",
    "    #     best_reg_param = reg_param_range[np.argmax(score_CV)]\n",
    "\n",
    "    # Create and fit model with best regularization parameter\n",
    "        if regularization_type == 'l2':\n",
    "            model = Log_Reg(penalty = regularization_type, C = best_reg_param)\n",
    "        elif regularization_type == 'l1':\n",
    "            model = Log_Reg(penalty = regularization_type, C = best_reg_param)\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    # Print some results\n",
    "    if print_results:\n",
    "        print '\\n', regularization_type,  'regression model created'\n",
    "        print 'Best regularization parameter:', best_reg_param\n",
    "        print 'Train R^2: ', format(model.score(x, y), '.2f')\n",
    "        print 'CV R^2: ', format(best_score, '.2f')\n",
    "    \n",
    "    # Visualize search\n",
    "    if visualize_results:\n",
    "        visualize_regularized_regression(score_train, score_CV, reg_param_range, regularization_type + ' regression')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-04T14:54:38.633841",
     "start_time": "2016-12-04T14:54:38.630503"
    }
   },
   "source": [
    "### Random Forest - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.723498",
     "start_time": "2016-12-14T22:23:20.700218"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_random_forest_regression_model(x, y, print_results = False):\n",
    "    \"\"\"\n",
    "    Create random forest model that best fits the x and y dataframe\n",
    "    \n",
    "    Input:\n",
    "    - x: Dataframe (m x n) with m observations and n predictors\n",
    "    - y: Dataframe (m x 1) with m observations and 1 independent variable\n",
    "    - print results: Optional to show best regularization parameter and associated CV and Training set scores\n",
    "    \n",
    "    Output:\n",
    "    - model: Fitted random forest model optimized for the tuning parameters number of trees and tree depth\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create evaluation parameter range    \n",
    "    n_trees = np.arange(10, 150, 20)  # Trees and depth are explored on an exponentially growing space,\n",
    "    depths = np.arange(2, 10)   # since it is assumed that trees and depth will add accuracy in a decaying fashion.\n",
    "\n",
    "    # Create list to keep track of scores\n",
    "    score_train = []\n",
    "    score_CV = []\n",
    "    \n",
    "    # Keep track of current best score\n",
    "    best_score = 0\n",
    "    \n",
    "    # Iterate over number of trees\n",
    "    for trees in n_trees:\n",
    "        \n",
    "        # Iterete over different depths\n",
    "        for depth in depths:\n",
    "            \n",
    "            # Create random forest model\n",
    "            model = Reg_Forest(n_estimators = trees, max_depth = depth)\n",
    "\n",
    "            # Get cross validation score \n",
    "            average_score = np.mean(CV(model, x, y, cv = 5))\n",
    "            \n",
    "            if average_score > best_score:\n",
    "                best_score = average_score\n",
    "                best_depth = depth\n",
    "                best_trees = trees\n",
    "           \n",
    "    # Recreate best model and fit it using the whole training data\n",
    "    model = Reg_Forest(n_estimators = best_trees, max_depth = best_depth)\n",
    "    \n",
    "    # Fit model using entire training set\n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # Print some results\n",
    "    if print_results:\n",
    "        print '\\n Random Forest Model created'\n",
    "        print 'Best number of trees: ', best_trees\n",
    "        print 'Best depth: ', best_depth\n",
    "        print 'Train R^2: ', format(model.score(x, y), '.2f')\n",
    "        print 'CV R^2: ', format(best_score, '.2f')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-04T14:54:38.633841",
     "start_time": "2016-12-04T14:54:38.630503"
    }
   },
   "source": [
    "### Random Forest -  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.750452",
     "start_time": "2016-12-14T22:23:20.725712"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_random_forest_classification_model(x, y, print_results = False):\n",
    "    \"\"\"\n",
    "    Create random forest model that best fits the x and y dataframe\n",
    "    \n",
    "    Input:\n",
    "    - x: Dataframe (m x n) with m observations and n predictors\n",
    "    - y: Dataframe (m x 1) with m observations and 1 independent variable\n",
    "    - print results: Optional to show best regularization parameter and associated CV and Training set scores\n",
    "    \n",
    "    Output:\n",
    "    - model: Fitted random forest model optimized for the tuning parameters number of trees and tree depth\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create evaluation parameter range    \n",
    "    n_trees = np.arange(10, 150, 20)  # Trees and depth are explored on an exponentially growing space,\n",
    "    depths = np.arange(2, 10)   # since it is assumed that trees and depth will add accuracy in a decaying fashion.\n",
    "\n",
    "    # Create list to keep track of scores\n",
    "    score_train = []\n",
    "    score_CV = []\n",
    "    \n",
    "    # Keep track of current best score\n",
    "    best_score = 0\n",
    "    \n",
    "    # Iterate over number of trees\n",
    "    for trees in n_trees:\n",
    "        \n",
    "        # Iterete over different depths\n",
    "        for depth in depths:\n",
    "            \n",
    "            # Create random forest model\n",
    "            model = Cls_Forest(n_estimators = trees, max_depth = depth)\n",
    "\n",
    "            # Get cross validation score \n",
    "            average_score = np.mean(CV(model, x, y, cv = 5))\n",
    "            \n",
    "            if average_score > best_score:\n",
    "                best_score = average_score\n",
    "                best_depth = depth\n",
    "                best_trees = trees\n",
    "           \n",
    "    # Recreate best model and fit it using the whole training data\n",
    "    model = Cls_Forest(n_estimators = best_trees, max_depth = best_depth)\n",
    "    \n",
    "    # Fit model using entire training set\n",
    "    model.fit(x,y)\n",
    "    \n",
    "    # Print some results\n",
    "    if print_results:\n",
    "        print '\\n Random Forest Model created'\n",
    "        print 'Best number of trees: ', best_trees\n",
    "        print 'Best depth: ', best_depth\n",
    "        print 'Train R^2: ', format(model.score(x, y), '.2f')\n",
    "        print 'CV R^2: ', format(best_score, '.2f')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-04T14:54:38.633841",
     "start_time": "2016-12-04T14:54:38.630503"
    }
   },
   "source": [
    "#### Visualize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.765152",
     "start_time": "2016-12-14T22:23:20.752878"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_importance_features_list(random_forest_model, x_columns, visualize_results = False, title = 'Relative importance of Each Feature'):\n",
    "    \"\"\"\n",
    "    Create a list of most important features for the random forest\n",
    "    \n",
    "    Input:\n",
    "    - random_forest_model: a fitted random forest model\n",
    "    - x_columns: names of the columns of the fitted dataframe\n",
    "    - visualize_results: Optional to show graph of most imortant features\n",
    "    - title: String with title above plot\n",
    "    \n",
    "    Output:\n",
    "    - features_list: Dataframe of features and relative importance\n",
    "    - Optional: Visualization of features\n",
    "    \"\"\" \n",
    "    \n",
    "    # Create importance list\n",
    "    features_list = random_forest_model.feature_importances_\n",
    "    \n",
    "    # Get matching column names\n",
    "    name_list = x_columns\n",
    "    \n",
    "    features_list, name_list = zip(*sorted(zip(features_list, name_list)))\n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.barh(range(len(name_list)), features_list, align='center')\n",
    "    plt.yticks(range(len(name_list)), name_list)\n",
    "    plt.xlabel('Relative Importance in the Random Forest')\n",
    "    plt.ylabel('Predictor')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "#     importance_list_df = pd.DataFrame(data = features_list, columns = ['Importance'], index = name_list)\n",
    "    \n",
    "#     return importance_list_df.sort_values('Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Execution\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.786194",
     "start_time": "2016-12-14T22:23:20.766933"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pixel_ID', 'lat', 'lon', 'cf', 'directionality', 'population',\n",
       "       'road_connectivity', 'road_capacity', 'road_capacity_incl_highway',\n",
       "       'road_capacity_highway', 'road_distance',\n",
       "       'road_distance_incl_highway', 'road_distance_highway',\n",
       "       'oneway_distance', 'oneway_dist_percentage', 'oneway_capacity',\n",
       "       'oneway_percentage', 'highway_bin', 'bridge_bin', 'tunnel_bin',\n",
       "       'rail_bin', 'waterway_bin', 'waterway_distance', 'cf_category'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels_file = 'SP_pixel_list_cf_predictors_v2_categories.csv'\n",
    "# pixels_file = 'SFO_BayArea_pixel_list_cf_predictors.csv'\n",
    "pixels_data_df = load_data(pixels_file)\n",
    "pixels_data_df.head()\n",
    "pixels_data_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression indicator\n",
    "\n",
    "Since we have implemented both regression as well classification functions, we have specified a parameter that serves as indicator if the regression or the classification models should be run. This depends on the input dataset used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.791622",
     "start_time": "2016-12-14T22:23:20.788100"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract relevant columns (predictors and response variable only)\n",
    "regression = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictor selection\n",
    "\n",
    "Based on this regression flag, we determine what predictors we include in our analysis. This subset of predictors is chosen to account for multicollinearity issues. Obviously, the independent variable is different in the cases of regression and classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.826282",
     "start_time": "2016-12-14T22:23:20.793307"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Road Connectivity</th>\n",
       "      <th>Road Capacity</th>\n",
       "      <th>Highway Capacity</th>\n",
       "      <th>Road Distance</th>\n",
       "      <th>Highway Distance</th>\n",
       "      <th>One-Way Distance</th>\n",
       "      <th>Bridge</th>\n",
       "      <th>Tunnel</th>\n",
       "      <th>Rail</th>\n",
       "      <th>Waterway Distance</th>\n",
       "      <th>Circuity Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>19.721551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.780871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.640166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>67.709612</td>\n",
       "      <td>0.648042</td>\n",
       "      <td>18.681803</td>\n",
       "      <td>0.648042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.371079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.370412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.342603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.532384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.292541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.370412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.342603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.677605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population  Road Connectivity  Road Capacity  Highway Capacity  \\\n",
       "0           0                 12      19.721551          0.000000   \n",
       "1           0                  8      67.709612          0.648042   \n",
       "2           0                  0       0.000000         65.370412   \n",
       "3           0                  0       0.000000          0.000000   \n",
       "4           0                  0       0.000000         65.370412   \n",
       "\n",
       "   Road Distance  Highway Distance  One-Way Distance  Bridge  Tunnel  Rail  \\\n",
       "0      10.780871          0.000000          0.559462       0       0     0   \n",
       "1      18.681803          0.648042          0.000000       0       0     0   \n",
       "2       0.000000         16.342603          0.000000       0       0     0   \n",
       "3       0.000000          0.000000          0.000000       0       0     0   \n",
       "4       0.000000         16.342603          0.000000       0       0     0   \n",
       "\n",
       "   Waterway Distance  Circuity Factor  \n",
       "0                0.0         2.640166  \n",
       "1                0.0         1.371079  \n",
       "2                0.0         1.532384  \n",
       "3                0.0         3.292541  \n",
       "4                0.0         2.677605  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if regression:\n",
    "\n",
    "    xy_df = pixels_data_df[['population',\n",
    "                            'road_connectivity', \n",
    "                            'road_capacity','road_capacity_highway', \n",
    "                            'road_distance','road_distance_highway',\n",
    "                            'oneway_distance',\n",
    "                            'bridge_bin', 'tunnel_bin',\n",
    "                            'rail_bin', 'waterway_distance', 'cf']]\n",
    "else:\n",
    "\n",
    "    xy_df = pixels_data_df[['population',\n",
    "                            'road_connectivity', \n",
    "                            'road_capacity','road_capacity_highway', \n",
    "                            'road_distance','road_distance_highway',\n",
    "                            'oneway_distance',\n",
    "                            'bridge_bin', 'tunnel_bin',\n",
    "                            'rail_bin', 'waterway_distance', 'cf_category']]\n",
    "\n",
    "# Rename columns\n",
    "xy_df.columns = ['Population',\n",
    "                     'Road Connectivity', \n",
    "                     'Road Capacity','Highway Capacity', \n",
    "                     'Road Distance','Highway Distance',\n",
    "                     'One-Way Distance',\n",
    "                     'Bridge', \n",
    "                     'Tunnel',\n",
    "                     'Rail', \n",
    "                     'Waterway Distance', 'Circuity Factor']\n",
    "\n",
    "xy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.834756",
     "start_time": "2016-12-14T22:23:20.828619"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors List\n",
      "['Population', 'Road Connectivity', 'Road Capacity', 'Highway Capacity', 'Road Distance', 'Highway Distance', 'One-Way Distance', 'Bridge', 'Tunnel', 'Rail', 'Waterway Distance']\n"
     ]
    }
   ],
   "source": [
    "# Create list of predictor names\n",
    "predictors = list(xy_df.columns.values)\n",
    "predictors = predictors[:-1]\n",
    "print 'Predictors List'\n",
    "print predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter pixels\n",
    "\n",
    "Since we are analyzing the circuity factor in urban areas, we only want to include urban areas in our analysis. Therefore, we filter pixels that don't exceed a certain population bound. We also exclude pixels that do not have any roads or intersections for obvious reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.849781",
     "start_time": "2016-12-14T22:23:20.837140"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter data based on:  Population  - threshold value:  5000\n",
      "Filter data based on:  Road Distance  - threshold value:  0\n",
      "Filter data based on:  Road Connectivity  - threshold value:  0\n",
      "Observations after filtering:  1365\n"
     ]
    }
   ],
   "source": [
    "# Set filters\n",
    "# Set filter columns and respective values\n",
    "filter_columns = ['Population', 'Road Distance', 'Road Connectivity']\n",
    "filter_values = [ 5000, 0, 0]\n",
    "\n",
    "# Iterate over all columns with a minimum filter\n",
    "for i in range(len(filter_values)):\n",
    "    print 'Filter data based on: ', filter_columns[i], ' - threshold value: ', filter_values[i]\n",
    "    xy_df = xy_df[xy_df[filter_columns[i]] > filter_values[i]]\n",
    "\n",
    "print 'Observations after filtering: ', len(xy_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data\n",
    "\n",
    "Split the data set in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.859779",
     "start_time": "2016-12-14T22:23:20.851745"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in (training, testing) set:  910 , 455\n"
     ]
    }
   ],
   "source": [
    "xy_train, xy_test = split(xy_df, 1/3)\n",
    "print 'Observations in (training, testing) set: ', len(xy_train), ',', len(xy_test)\n",
    "# xy_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.868012",
     "start_time": "2016-12-14T22:23:20.861570"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract x and y\n",
    "x_train, y_train = create_x_y(xy_train)\n",
    "x_test, y_test = create_x_y(xy_test)\n",
    "\n",
    "#Standarize the data\n",
    "x_train_std, scaler = standardize_data(x_train)\n",
    "x_test_std = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a matrix to keep track of model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.873799",
     "start_time": "2016-12-14T22:23:20.870149"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create matrix to keep track of model scores\n",
    "# For each regression model, track: Model_type, R^2 train, R^2 test, key_paramters\n",
    "# For each regression model, track: Model_type, accuracy train, accuracy test, key_paramters\n",
    "\n",
    "model_results_matrix = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.925866",
     "start_time": "2016-12-14T22:23:20.875669"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.43\n",
      "Test R^2:   0.43\n",
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.43\n",
      "Test R^2:   0.43\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    lin_reg_model = create_linear_regression_model(x_train, y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(lin_reg_model.score(x_test, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Linear Regression (No Standardization)', \n",
    "            lin_reg_model.score(x_train, y_train),\n",
    "            lin_reg_model.score(x_test, y_test),\n",
    "            ''\n",
    "        ])\n",
    "\n",
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    lin_reg_model = create_linear_regression_model(x_train_std, y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(lin_reg_model.score(x_test_std, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Linear Regression', \n",
    "            lin_reg_model.score(x_train_std, y_train),\n",
    "            lin_reg_model.score(x_test_std, y_test),\n",
    "            ''\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation with StatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.933959",
     "start_time": "2016-12-14T22:23:20.928528"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors List\n",
      "['Population', 'Road Connectivity', 'Road Capacity', 'Highway Capacity', 'Road Distance', 'Highway Distance', 'One-Way Distance', 'Bridge', 'Tunnel', 'Rail', 'Waterway Distance']\n"
     ]
    }
   ],
   "source": [
    "print 'Predictors List'\n",
    "print [predictor for predictor in predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:20.945433",
     "start_time": "2016-12-14T22:23:20.936167"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_sm = sm.add_constant(x_train_std)\n",
    "validation = OLS(y_train, x_train_sm).fit()\n",
    "# print validation.summary()\n",
    "# validation.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward stepwise subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:21.048516",
     "start_time": "2016-12-14T22:23:20.947865"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-wise forward subset selection:\n",
      "Train R^2:  0.43\n",
      "Predictors:  ['Road Connectivity', 'Highway Distance', 'Bridge', 'Road Distance', 'Road Capacity', 'Population', 'Rail', 'Waterway Distance', 'Tunnel', 'Highway Capacity']\n",
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.43\n",
      "Test R^2:   0.43\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    # Select best predictors\n",
    "    forward_predictors = forward_stepwise_subset_selection(x_train_std, y_train, predictors, print_results = True)\n",
    "    \n",
    "    # Run model with best predictors\n",
    "    lin_reg_model = create_linear_regression_model(x_train_std[:, forward_predictors], y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(lin_reg_model.score(x_test_std[:, forward_predictors], y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Forward Stepwise Regression', \n",
    "            lin_reg_model.score(x_train_std[:, forward_predictors], y_train),\n",
    "            lin_reg_model.score(x_test_std[:, forward_predictors], y_test),\n",
    "            'Number of predictors: ' + str(len(forward_predictors))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward stepwise subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:21.147432",
     "start_time": "2016-12-14T22:23:21.050830"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-wise backward subset selection:\n",
      "Train R^2:  0.43\n",
      "Subset of best predictors:  ['Population', 'Road Connectivity', 'Road Capacity', 'Highway Capacity', 'Road Distance', 'Highway Distance', 'Bridge', 'Tunnel', 'Rail', 'Waterway Distance']\n",
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.43\n",
      "Test R^2:   0.43\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    # Select best predictors\n",
    "    backward_predictors = backward_stepwise_subset_selection(x_train_std, y_train, predictors, print_results = True)\n",
    "\n",
    "    # Run model with best predictors\n",
    "    lin_reg_model = create_linear_regression_model(x_train_std[:, backward_predictors], y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(lin_reg_model.score(x_test_std[:, backward_predictors], y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Backward Stepwise Regression', \n",
    "            lin_reg_model.score(x_train_std[:, backward_predictors], y_train),\n",
    "            lin_reg_model.score(x_test_std[:, backward_predictors], y_test),\n",
    "            'Number of predictors: ' + str(len(backward_predictors))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### Quadratic Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:21.157697",
     "start_time": "2016-12-14T22:23:21.149800"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create polynomial terms\n",
    "x_train_transf = polynomial_transf(x_train_std, 2)\n",
    "x_test_transf = polynomial_transf(x_test_std, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:21.177529",
     "start_time": "2016-12-14T22:23:21.160244"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.51\n",
      "Test R^2:   0.31\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    # Run polynomial model\n",
    "    poly_reg_model = create_linear_regression_model(x_train_transf, y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(poly_reg_model.score(x_test_transf, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Polynomial Regression - ' + str(2), \n",
    "            poly_reg_model.score(x_train_transf, y_train),\n",
    "            poly_reg_model.score(x_test_transf, y_test),\n",
    "            ''\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward stepwise subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:26.316309",
     "start_time": "2016-12-14T22:23:21.179978"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.50\n",
      "Test R^2:   0.31\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    # Select best predictors\n",
    "    forward_predictors = forward_stepwise_subset_selection(x_train_transf, y_train, predictors, print_results = False)\n",
    "    \n",
    "    # Run model with best predictors\n",
    "    lin_reg_model = create_linear_regression_model(x_train_transf[:, forward_predictors], y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(lin_reg_model.score(x_test_transf[:, forward_predictors], y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Forward Stepwise Polynomial Regression', \n",
    "            lin_reg_model.score(x_train_transf[:, forward_predictors], y_train),\n",
    "            lin_reg_model.score(x_test_transf[:, forward_predictors], y_test),\n",
    "            'Number of predictors: ' + str(len(forward_predictors))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-06T19:58:07.379348",
     "start_time": "2016-12-07T00:57:16.836Z"
    }
   },
   "source": [
    "#### Backward stepwise subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:33.680728",
     "start_time": "2016-12-14T22:23:26.319095"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.50\n",
      "Test R^2:   0.32\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    # Select best predictors\n",
    "    backward_predictors = backward_stepwise_subset_selection(x_train_transf, y_train, predictors, print_results = False)\n",
    "\n",
    "    # Run model with best predictors\n",
    "    lin_reg_model = create_linear_regression_model(x_train_transf[:, backward_predictors], y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(lin_reg_model.score(x_test_transf[:, backward_predictors], y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Backward Stepwise Polynomial Regression', \n",
    "            lin_reg_model.score(x_train_transf[:, backward_predictors], y_train),\n",
    "            lin_reg_model.score(x_test_transf[:, backward_predictors], y_test),\n",
    "            'Number of predictors: ' + str(len(backward_predictors))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:33.904328",
     "start_time": "2016-12-14T22:23:33.682324"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge regression model created\n",
      "Best regularization parameter: 100\n",
      "Train R^2:  0.47\n",
      "CV R^2:  0.35\n",
      "Test R^2:   0.37\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    ridge_poly_reg_model = create_regularized_regression_model(x_train_transf, y_train, regularization_type = 'Ridge', print_results = True, visualize_results = False)\n",
    "    print 'Test R^2:  ', format(ridge_poly_reg_model.score(x_test_transf, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Polynomial Ridge Regression - ' + str(2), \n",
    "            ridge_poly_reg_model.score(x_train_transf, y_train),\n",
    "            ridge_poly_reg_model.score(x_test_transf, y_test),\n",
    "            'alpha: ' + str(ridge_poly_reg_model.alpha)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:34.986936",
     "start_time": "2016-12-14T22:23:33.906261"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso regression model created\n",
      "Best regularization parameter: 0.01\n",
      "Train R^2:  0.47\n",
      "CV R^2:  0.38\n",
      "Test R^2:   0.39\n"
     ]
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    lasso_poly_reg_model = create_regularized_regression_model(x_train_transf, y_train, regularization_type = 'Lasso', print_results = True, visualize_results = False)\n",
    "    print 'Test R^2:  ', format(lasso_poly_reg_model.score(x_test_transf, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Polynomial Lasso Regression - ' + str(2), \n",
    "            lasso_poly_reg_model.score(x_train_transf, y_train),\n",
    "            lasso_poly_reg_model.score(x_test_transf, y_test),\n",
    "            'alpha: ' + str(lasso_poly_reg_model.alpha)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Validation with StatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:34.994101",
     "start_time": "2016-12-14T22:23:34.989198"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors List\n",
      "['Population', 'Road Connectivity', 'Road Capacity', 'Highway Capacity', 'Road Distance', 'Highway Distance', 'One-Way Distance', 'Bridge', 'Tunnel', 'Rail', 'Waterway Distance']\n"
     ]
    }
   ],
   "source": [
    "print 'Predictors List'\n",
    "print [predictor for predictor in predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:35.009957",
     "start_time": "2016-12-14T22:23:34.996518"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x_train_transf_sm = sm.add_constant(x_train_transf)\n",
    "validation_poly = OLS(y_train, x_train_transf).fit()\n",
    "# print validation_poly.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:35.032509",
     "start_time": "2016-12-14T22:23:35.012164"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear regression model created\n",
      "Train R^2:  0.51\n",
      "Test R^2:   0.31\n"
     ]
    }
   ],
   "source": [
    "# Create non standardized polynomial predictors\n",
    "x_train_transf_nostd = polynomial_transf(x_train, 2)\n",
    "x_test_transf_nostd = polynomial_transf(x_test, 2)\n",
    "\n",
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    # Run polynomial model\n",
    "    poly_reg_model_nostd = create_linear_regression_model(x_train_transf_nostd, y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(poly_reg_model_nostd.score(x_test_transf_nostd, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Polynomial Regression (Non Standardization) - ' + str(2), \n",
    "            poly_reg_model_nostd.score(x_train_transf_nostd, y_train),\n",
    "            poly_reg_model_nostd.score(x_test_transf_nostd, y_test),\n",
    "            ''\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:35.047195",
     "start_time": "2016-12-14T22:23:35.035444"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validation_poly_nostd = sm.OLS(y_train, x_train_transf_nostd).fit()\n",
    "# print validation_poly_nostd.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:35.057504",
     "start_time": "2016-12-14T22:23:35.049582"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only run model in case of classification\n",
    "if not regression:\n",
    "    log_reg_model = create_logistic_regression_model(x_train_std, y_train, regularization_type='l2', print_results = True)\n",
    "    print 'Test R^2:  ', format(log_reg_model.score(x_test_std, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Logistic Regression - Std', \n",
    "            log_reg_model.score(x_train_std, y_train),\n",
    "            log_reg_model.score(x_test_std, y_test),\n",
    "            'C: ' + str(log_reg_model.C)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:35.067429",
     "start_time": "2016-12-14T22:23:35.059744"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only run model in case of classification\n",
    "if not regression:\n",
    "    poly_log_reg_model = create_logistic_regression_model(x_train_transf, y_train, regularization_type='l2', print_results = True)\n",
    "    print 'Test R^2:  ', format(poly_log_reg_model.score(x_test_transf, y_test), '0.2f')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Polynomial Logistic Regression - ' + str(2),\n",
    "            poly_log_reg_model.score(x_train_transf, y_train),\n",
    "            poly_log_reg_model.score(x_test_transf, y_test),\n",
    "            'C: ' + str(poly_log_reg_model.C)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random forest model - No Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:35.077263",
     "start_time": "2016-12-14T22:23:35.070366"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only run model in case of classification\n",
    "if not regression:\n",
    "    forest_cls_model_nostd = create_random_forest_classification_model(x_train, y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(forest_cls_model_nostd.score(x_test, y_test), '0.2f')\n",
    "    features_list_cls = create_importance_features_list(forest_cls_model_nostd, predictors, visualize_results = True, title = 'Relative importance of Predictors')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Random Forest Classification', \n",
    "            forest_cls_model_nostd.score(x_train, y_train),\n",
    "            forest_cls_model_nostd.score(x_test, y_test),\n",
    "            'trees, depth: ' + str(forest_cls_model_nostd.n_estimators) + ',' + str(forest_cls_model_nostd.max_depth)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:23:35.083301",
     "start_time": "2016-12-14T22:23:35.079785"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print forest_cls_model_nostd.feature_importances_\n",
    "# predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:25:08.729055",
     "start_time": "2016-12-14T22:23:35.085469"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Random Forest Model created\n",
      "Best number of trees:  90\n",
      "Best depth:  7\n",
      "Train R^2:  0.76\n",
      "CV R^2:  0.42\n",
      "Test R^2:   0.33\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyQAAAK/CAYAAACY3AkwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/werNzrw2lACIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103fbb590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Only run model in case of regression\n",
    "if regression:\n",
    "    forest_reg_model_nostd = create_random_forest_regression_model(x_train, y_train, print_results = True)\n",
    "    print 'Test R^2:  ', format(forest_reg_model_nostd.score(x_test, y_test), '0.2f')\n",
    "    features_list_reg = create_importance_features_list(forest_reg_model_nostd, predictors, visualize_results = True, title = 'Relative importance of Each Feature')\n",
    "    # Append model results to model results matrix\n",
    "    model_results_matrix.append([\n",
    "            'Random Forest Regression', \n",
    "            forest_reg_model_nostd.score(x_train, y_train),\n",
    "            forest_reg_model_nostd.score(x_test, y_test),\n",
    "            'trees, depth: ' + str(forest_reg_model_nostd.n_estimators) + ',' + str(forest_reg_model_nostd.max_depth)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:25:08.753219",
     "start_time": "2016-12-14T22:25:08.730855"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Relevant Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression (No Standardization)</td>\n",
       "      <td>0.434377</td>\n",
       "      <td>0.428022</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.434377</td>\n",
       "      <td>0.428022</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forward Stepwise Regression</td>\n",
       "      <td>0.433912</td>\n",
       "      <td>0.427306</td>\n",
       "      <td>Number of predictors: 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Backward Stepwise Regression</td>\n",
       "      <td>0.433912</td>\n",
       "      <td>0.427306</td>\n",
       "      <td>Number of predictors: 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Polynomial Regression - 2</td>\n",
       "      <td>0.506048</td>\n",
       "      <td>0.309171</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Forward Stepwise Polynomial Regression</td>\n",
       "      <td>0.502224</td>\n",
       "      <td>0.309727</td>\n",
       "      <td>Number of predictors: 49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Backward Stepwise Polynomial Regression</td>\n",
       "      <td>0.500897</td>\n",
       "      <td>0.320743</td>\n",
       "      <td>Number of predictors: 42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Polynomial Ridge Regression - 2</td>\n",
       "      <td>0.469893</td>\n",
       "      <td>0.370441</td>\n",
       "      <td>alpha: 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial Lasso Regression - 2</td>\n",
       "      <td>0.472965</td>\n",
       "      <td>0.391058</td>\n",
       "      <td>alpha: 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polynomial Regression (Non Standardization) - 2</td>\n",
       "      <td>0.506048</td>\n",
       "      <td>0.309142</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest Regression</td>\n",
       "      <td>0.758931</td>\n",
       "      <td>0.331457</td>\n",
       "      <td>trees, depth: 90,7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Model  Train Score  Test Score  \\\n",
       "0            Linear Regression (No Standardization)     0.434377    0.428022   \n",
       "1                                 Linear Regression     0.434377    0.428022   \n",
       "2                       Forward Stepwise Regression     0.433912    0.427306   \n",
       "3                      Backward Stepwise Regression     0.433912    0.427306   \n",
       "4                         Polynomial Regression - 2     0.506048    0.309171   \n",
       "5            Forward Stepwise Polynomial Regression     0.502224    0.309727   \n",
       "6           Backward Stepwise Polynomial Regression     0.500897    0.320743   \n",
       "7                   Polynomial Ridge Regression - 2     0.469893    0.370441   \n",
       "8                   Polynomial Lasso Regression - 2     0.472965    0.391058   \n",
       "9   Polynomial Regression (Non Standardization) - 2     0.506048    0.309142   \n",
       "10                         Random Forest Regression     0.758931    0.331457   \n",
       "\n",
       "         Relevant Parameters  \n",
       "0                             \n",
       "1                             \n",
       "2   Number of predictors: 10  \n",
       "3   Number of predictors: 10  \n",
       "4                             \n",
       "5   Number of predictors: 49  \n",
       "6   Number of predictors: 42  \n",
       "7                 alpha: 100  \n",
       "8                alpha: 0.01  \n",
       "9                             \n",
       "10        trees, depth: 90,7  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores = pd.DataFrame(data = model_results_matrix, columns = ['Model', 'Train Score', 'Test Score', 'Relevant Parameters'])\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-14T22:25:08.965498",
     "start_time": "2016-12-14T22:25:08.755601"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAIPCAYAAADjB61rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqgwFkiQJMBRIkqTq/wMx3uccEiFYDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113340f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute matrix of correlation coefficients\n",
    "corr_matrix = np.corrcoef(x_train.T)\n",
    "\n",
    "# Display heat map \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.pcolor(corr_matrix)\n",
    "\n",
    "ax.set_title('Heatmap of correlation matrix')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "940px",
    "left": "0px",
    "right": "1607.5px",
    "top": "106px",
    "width": "254px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
